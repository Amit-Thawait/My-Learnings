Machine Learning :-
===================
Machine Learning Algorithms :
1) Supervised learning
2) Unsupervised learning

Supervised Learning :
---------------------
In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.

Supervised learning problems are categorized into "regression" and "classification" problems.

In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. 
In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories.

Example 1:

Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem.

We could turn this example into a classification problem by instead making our output about whether the house "sells for more or less than the asking price." Here we are classifying the houses based on price into two discrete categories.

Example 2:

(a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture

(b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign.

Unsupervised Learning :
-----------------------
Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables.

We can derive this structure by clustering the data based on relationships among the variables in the data.

With unsupervised learning there is no feedback based on the prediction results.

Example:

Clustering: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.

Non-clustering: The "Cocktail Party Algorithm", allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party).

Octave commands :
=================
1 == 2 
=> false

% is used to write comment

~= : represents "not equal to"

1 && 0 => 0

1 || 0 => 1

xor(1, 0) => 1

a = 3
Above command prints output in the terminal
=> a = 3

If you want to supress output in the terminal use semicolon at the end.
a = 3;

a = pi;

disp(a)

disp(sprintf('2 decimals: %0.2f', a))

format long
% prints output for numbers with more decimal places

format short
% prints output for numbers with 4 decimal places

Matrix A
A = [1 2; 3 4; 5 6]
prints

A =
   1   2
   3   4
   5   6

We can also set a starting number and tell how to increment it and when to end. For ex:
octave:8> v = 1:0.1:2
v =

    1.0000    1.1000    1.2000    1.3000    1.4000    1.5000    1.6000    1.7000    1.8000    1.9000    2.0000

octave:9> v = 1:6
v =

   1   2   3   4   5   6

For Identity matrix
octave:17> I = eye(4)
I =

Diagonal Matrix

   1   0   0   0
   0   1   0   0
   0   0   1   0
   0   0   0   1

To know more about any command use help command

help eye

who command gives variables in the current scope
octave:18> who
Variables in the current scope:

A    I    a    ans  v    w

whos gives the detailed view of all the variables.

------------------------------------------------------------------------
For Ubuntu, switch to gnuplot if graphs are not showing up properly

graphics_toolkit("gnuplot")
------------------------------------------------------------------------

The Problem of Overfitting :-
=============================
Underfitting, or high bias, is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. At the other extreme, overfitting, or high variance, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.

This terminology is applied to both linear and logistic regression. There are two main options to address the issue of overfitting:

1) Reduce the number of features:
- Manually select which features to keep.
- Use a model selection algorithm (studied later in the course).

2) Regularization
- Keep all the features, but reduce the magnitude of parameters θj.
- Regularization works well when we have a lot of slightly useful features.

Common terminologies :
======================
Model: A mathematical representation of a real world process; a predictive model forecasts a future outcome based on past behaviors.

Algorithm: A set of rules used to make a calculation or solve a problem.

Training: The process of creating a model from the training data. The data is fed into the training algorithm, which learns a representation for the problem and produces a model. Also called “learning.”

Regression: A prediction method whose output is a real number, that is, a value that represents a quantity along a line. Example: predicting the temperature of an engine or the revenue of a company.

Classification: A prediction method that assigns each data point to a predefined category, e.g., a type of operating system.

Target: In statistics, it is called the dependent variable; it is the output of the model or the variable you wish to predict.

Training set: A dataset used to find potentially predictive relationships that will be used to create a model.

Test set: A data set, separate from the training set but with the same structure, used to measure and benchmark the performance of various models.

Feature: Also known as an independent variable or a predictor variable, a feature is an observable quantity,recorded and used by a prediction model. You can also engineer features by combining them or adding new information to them.

Overfitting: A situation in which a model that is too complex for the data has been trained to predict the target. This leads to an overly specialized model, which makes predictions that do not reflect the reality of the underlying relationship between the features and target.

-----------------------------------------------------------------------------------

=> It's not who has the best algorithm that wins. It's who has the most data.

-----------------------------------------------------------------------------------

How do I know which model to choose for my problem ?

Same as for regression models, you first need to figure out whether your problem is linear or non linear.

If your problem is linear, you should go for Logistic Regression or SVM.

If your problem is non linear, you should go for K-NN, Naive Bayes, Decision Tree or Random Forest.

Then from a business point of view, you would rather use:

- Logistic Regression or Naive Bayes when you want to rank your predictions by their probability. For example if you want to rank your customers from the highest probability that they buy a certain product, to the lowest probability. Eventually that allows you to target your marketing campaigns. And of course for this type of business problem, you should use Logistic Regression if your problem is linear, and Naive Bayes if your problem is non linear.

- SVM when you want to predict to which segment your customers belong to. Segments can be any kind of segments, for example some market segments you identified earlier with clustering.

- Decision Tree when you want to have clear interpretation of your model results,

- Random Forest when you are just looking for high performance with less need for interpretation.   
